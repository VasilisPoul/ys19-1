{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "task_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "source": [
        "### Imports"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKUQgBeCfieI"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gdown\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.pipeline import make_pipeline, Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import mean_squared_error,r2_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "source": [
        "### Download and read csv"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/u/0/uc?id=1VUn2WKkKeRXwH02K9bqH98KjPxrUmgXh&export=download\n",
            "To: /home/vasilis/projects/ai2-1st-assignement/HousingData.csv\n",
            "100%|██████████| 1.60M/1.60M [00:00<00:00, 3.20MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'HousingData.csv'"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "url = \"https://drive.google.com/u/0/uc?id=1VUn2WKkKeRXwH02K9bqH98KjPxrUmgXh&export=download\"\n",
        "filename = \"HousingData.csv\"\n",
        "gdown.download(url, filename, quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWle83HoVHe2"
      },
      "source": [
        "data = pd.read_csv(\"HousingData.csv\")\n",
        "data.dropna(inplace = True)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEs37x24nYg3"
      },
      "source": [
        "# data['Bedrooms Per Room'] = data['AveRooms'] / data['AveBedrms']\n",
        "# data['Occup Per Population'] = data['Population'] / data['AveOccup']\n",
        "\n",
        "y = data[['Median House Value']]\n",
        "X = data[['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']]\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "sc = StandardScaler()\n",
        "X = sc.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkhFUPddUCCF"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.28, random_state=42)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "source": [
        "### Stochastic Gradient Descent"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKma3qlTtgJS"
      },
      "source": [
        "reg =  SGDRegressor(loss='huber', max_iter=1000)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "source": [
        "### Cross Validation Score"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 48.7 ms, sys: 35 ms, total: 83.7 ms\nWall time: 1.91 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "score = cross_val_score(reg , X_train, y_train, scoring = \"neg_mean_squared_error\", cv = 10, n_jobs=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.767722592105251"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "tree_rmse_scores = np.sqrt(-score)\n",
        "tree_rmse_scores.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "reg.fit(X_train, y_train)\n",
        "\n",
        "y_pred = reg.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5362848219576521"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "r2_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "source": [
        "### Batch "
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVuXqwc1uVpG",
        "outputId": "264903d2-e2b9-440d-8565-fcb7a15acaf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "# function to compute predictions created for the current dataset\n",
        "# where y = w0 + w1*x1 +...+w8*x8 \n",
        "def predict(X, theta):\n",
        "  return np.dot(X, theta[0:8]) + theta[8]\n",
        "  \n",
        "# function to compute gradient of error \n",
        "def gradient(X, y, theta): \n",
        "    h = np.dot(X, theta) \n",
        "    grad = (2/X.shape[0])*np.dot(X.transpose(), (h - y)) \n",
        "    return grad \n",
        "  \n",
        "# function to compute MSE\n",
        "def cost(X, y, theta): \n",
        "    h = np.dot(X, theta) \n",
        "    J = np.dot((h - y).transpose(), (h - y)) \n",
        "    J /= X.shape[0]\n",
        "    return J[0] \n",
        "  \n",
        "# function to perform mini-batch gradient descent\n",
        "def gradientDescent(X, y, learning_rate = 0.1, steps=1000): \n",
        "    X = np.c_[ X, np.ones(X.shape[0]) ] #add a column of ones to X for the bias term\n",
        "    theta = np.zeros((X.shape[1], 1)) #create inital weights w0, w1..wd\n",
        "    error_list = []  \n",
        "    for s in range(steps):\n",
        "      theta = theta - learning_rate * gradient(X, y, theta) \n",
        "      error_list.append(cost(X, y, theta)) #useful for plotting changes when using different batch sizes\n",
        "  \n",
        "    return theta, error_list \n",
        "\n",
        "theta, errors = gradientDescent(X_train, y_train)\n",
        "# cost_ = cost(X_train, y_train, theta)\n",
        "y_pred = predict(X_test, theta)\n",
        "theta\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 8.14286526e-01],\n",
              "       [ 1.22891973e-01],\n",
              "       [-2.58237972e-01],\n",
              "       [ 2.96452200e-01],\n",
              "       [-8.85567085e-05],\n",
              "       [-7.33481403e-02],\n",
              "       [-8.92730259e-01],\n",
              "       [-8.56770213e-01],\n",
              "       [ 2.05752441e+00]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.594648235237901"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "r2_score(y_test,y_pred)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5436200487640566"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "mean_squared_error(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "source": [
        "### Mini Batch"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# function to compute predictions created for the current dataset\n",
        "# where y = w0 + w1*x1 +...+w8*x8 \n",
        "def predict(X, theta):\n",
        "  return np.dot(X, theta[0:8]) + theta[8]\n",
        "  \n",
        "# function to compute gradient of error \n",
        "def gradient(X, y, theta): \n",
        "    h = np.dot(X, theta) \n",
        "    grad = (2/X.shape[0])*np.dot(X.transpose(), (h - y)) \n",
        "    return grad \n",
        "  \n",
        "# function to compute MSE\n",
        "def cost(X, y, theta): \n",
        "    h = np.dot(X, theta) \n",
        "    J = np.dot((h - y).transpose(), (h - y)) \n",
        "    J /= X.shape[0]\n",
        "    return J[0] \n",
        "\n",
        "# function to create a list containing mini-batches \n",
        "def create_mini_batches(X, y, batch_size): \n",
        "    mini_batches = [] \n",
        "    data = np.hstack((X, y)) \n",
        "    # print(data.shape[0])\n",
        "    np.random.shuffle(data) \n",
        "    n_minibatches = data.shape[0] // batch_size \n",
        "    i = 0\n",
        "  \n",
        "    for i in range(n_minibatches + 1): \n",
        "        mini_batch = data[i * batch_size:(i + 1)*batch_size, :] \n",
        "        X_mini = mini_batch[:, :-1] \n",
        "        Y_mini = mini_batch[:, -1].reshape((-1, 1)) \n",
        "        mini_batches.append((X_mini, Y_mini)) \n",
        "    if data.shape[0] % batch_size != 0: \n",
        "        mini_batch = data[i * batch_size:data.shape[0]] \n",
        "        X_mini = mini_batch[:, :-1] \n",
        "        Y_mini = mini_batch[:, -1].reshape((-1, 1)) \n",
        "        mini_batches.append((X_mini, Y_mini)) \n",
        "    return mini_batches    \n",
        "  \n",
        "# function to perform mini-batch gradient descent\n",
        "def gradientDescent(X, y, learning_rate = 0.1, steps=1000, mini_batch_size=12): \n",
        "    X = np.c_[ X, np.ones(X.shape[0]) ] #add a column of ones to X for the bias term\n",
        "    theta = np.zeros((X.shape[1], 1)) #create inital weights w0, w1..wd\n",
        "    error_list = [] \n",
        "    counter = 1 \n",
        "    for s in range(steps):\n",
        "      mini_batches = create_mini_batches(X, y, mini_batch_size)  \n",
        "      # print(counter)\n",
        "      for mini_batch in mini_batches:\n",
        "        X_mini, y_mini = mini_batch\n",
        "        theta = theta - learning_rate * gradient(X_mini, y_mini, theta) \n",
        "        error_list.append(cost(X_mini, y_mini, theta)) #useful for plotting changes when using different batch sizes\n",
        "      counter+=1\n",
        "    return theta, error_list \n",
        "\n",
        "theta, errors = gradientDescent(X_train, y_train)\n",
        "y_pred = predict(X_test, theta)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-82438.46872992134"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "r2_score(y_test,y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}